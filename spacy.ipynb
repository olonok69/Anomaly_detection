{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spacy.ipynb",
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/olonok69/Anomaly_detection/blob/master/spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdEzfQpkXWvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0FZwMFGkiaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "!pip install -q spacy==2.3.2 \n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install -q nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ksUIxP6ImjIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import spacy\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "#spacy_stopwords.remove('a')\n",
        "import re\n",
        "import datetime\n",
        "from spacy.matcher import Matcher\n",
        "from spacy.util import filter_spans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT4a8OXp32A9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pattern = [{'POS': 'VERB', 'OP': '?'},\n",
        "           {'POS': 'ADV', 'OP': '*'},\n",
        "           {'POS': 'AUX', 'OP': '*'},\n",
        "           {'POS': 'VERB', 'OP': '+'}]\n",
        "\n",
        "# instantiate a Matcher instance\n",
        "matcher = Matcher(nlp.vocab)\n",
        "matcher.add(\"Verb phrase\", None, pattern)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1HCSPXWmZgu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap= pd.read_csv(\"/content/drive/My Drive/datasets/defects.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXGThWKsmvvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap['keywords']=datap['correct_Action']= datap['spacy_verb']=datap['actions']=datap['verb_Adj']=\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyC7yETwmubb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from collections import OrderedDict\n",
        "import numpy as np\n",
        "import datetime"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72qwMopdqSSw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextRank4Keyword():\n",
        "    \"\"\"Extract keywords from text\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.d = 0.85 # damping coefficient, usually is .85\n",
        "        self.min_diff = 1e-5 # convergence threshold\n",
        "        self.steps = 10 # iteration steps\n",
        "        self.node_weight = None # save keywords and its weight\n",
        "\n",
        "    \n",
        "    def set_stopwords(self, stopwords):  \n",
        "        \"\"\"Set stop words\"\"\"\n",
        "        for word in STOP_WORDS.union(set(stopwords)):\n",
        "            lexeme = nlp.vocab[word]\n",
        "            lexeme.is_stop = True\n",
        "    \n",
        "    def sentence_segment(self, doc, candidate_pos, lower):\n",
        "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
        "        sentences = []\n",
        "        for sent in doc.sents:\n",
        "            selected_words = []\n",
        "            for token in sent:\n",
        "                # Store words only with cadidate POS tag\n",
        "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
        "                    if lower is True:\n",
        "                        selected_words.append(token.text.lower())\n",
        "                    else:\n",
        "                        selected_words.append(token.text)\n",
        "            sentences.append(selected_words)\n",
        "        return sentences\n",
        "        \n",
        "    def get_vocab(self, sentences):\n",
        "        \"\"\"Get all tokens\"\"\"\n",
        "        vocab = OrderedDict()\n",
        "        i = 0\n",
        "        for sentence in sentences:\n",
        "            for word in sentence:\n",
        "                if word not in vocab:\n",
        "                    vocab[word] = i\n",
        "                    i += 1\n",
        "        return vocab\n",
        "    \n",
        "    def get_token_pairs(self, window_size, sentences):\n",
        "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
        "        token_pairs = list()\n",
        "        for sentence in sentences:\n",
        "            for i, word in enumerate(sentence):\n",
        "                for j in range(i+1, i+window_size):\n",
        "                    if j >= len(sentence):\n",
        "                        break\n",
        "                    pair = (word, sentence[j])\n",
        "                    if pair not in token_pairs:\n",
        "                        token_pairs.append(pair)\n",
        "        return token_pairs\n",
        "        \n",
        "    def symmetrize(self, a):\n",
        "        return a + a.T - np.diag(a.diagonal())\n",
        "    \n",
        "    def get_matrix(self, vocab, token_pairs):\n",
        "        \"\"\"Get normalized matrix\"\"\"\n",
        "        # Build matrix\n",
        "        vocab_size = len(vocab)\n",
        "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
        "        for word1, word2 in token_pairs:\n",
        "            i, j = vocab[word1], vocab[word2]\n",
        "            g[i][j] = 1\n",
        "            \n",
        "        # Get Symmeric matrix\n",
        "        g = self.symmetrize(g)\n",
        "        \n",
        "        # Normalize matrix by column\n",
        "        norm = np.sum(g, axis=0)\n",
        "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
        "        \n",
        "        return g_norm\n",
        "\n",
        "    \n",
        "    def get_keywords(self, number):\n",
        "        \"\"\"Print top number keywords\"\"\"\n",
        "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
        "        listw=[]\n",
        "        for i, (key, value) in enumerate(node_weight.items()):\n",
        "            #print(key + ' - ' + str(value))\n",
        "            listw.append(key)\n",
        "            if i > number:\n",
        "                break\n",
        "        return listw\n",
        "        \n",
        "    def analyze(self, text, \n",
        "                candidate_pos=['NOUN', 'PROPN'], \n",
        "                window_size=4, lower=False, stopwords=list()):\n",
        "        \"\"\"Main function to analyze text\"\"\"\n",
        "        \n",
        "        # Set stop words\n",
        "        self.set_stopwords(stopwords)\n",
        "        \n",
        "        # Pare text by spaCy\n",
        "        doc = nlp(text)\n",
        "        \n",
        "        # Filter sentences\n",
        "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
        "        \n",
        "        # Build vocabulary\n",
        "        vocab = self.get_vocab(sentences)\n",
        "        \n",
        "        # Get token_pairs from windows\n",
        "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
        "        \n",
        "        # Get normalized matrix\n",
        "        g = self.get_matrix(vocab, token_pairs)\n",
        "        \n",
        "        # Initionlization for weight(pagerank value)\n",
        "        pr = np.array([1] * len(vocab))\n",
        "        \n",
        "        # Iteration\n",
        "        previous_pr = 0\n",
        "        for epoch in range(self.steps):\n",
        "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
        "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
        "                break\n",
        "            else:\n",
        "                previous_pr = sum(pr)\n",
        "\n",
        "        # Get weight for each node\n",
        "        node_weight = dict()\n",
        "        for word, index in vocab.items():\n",
        "            node_weight[word] = pr[index]\n",
        "        \n",
        "        self.node_weight = node_weight"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ji0WnJ-lqSmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr4w = TextRank4Keyword()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxzPeXWaqSBl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lverbs=['VBD','VBP','VBZ', 'VB']\n",
        "lverbs2=[ 'VBG','VBN']\n",
        "\n",
        "def apply_get_actions(text, lverbs):\n",
        "    \n",
        "    tokens = word_tokenize(text)\n",
        "    out=[]\n",
        "    for i in range(len(tokens)):\n",
        "        if pos_tag(tokens)[i][1] in lverbs:\n",
        "            out.append(pos_tag(tokens)[i][0])\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "def apply_get_keywords(text, tr4w):\n",
        "    \n",
        "    tr4w.analyze(text.lower(), candidate_pos = ['PROPN',\"NOUN\"], window_size=5, lower=True)\n",
        "    out=tr4w.get_keywords(3)\n",
        "    if len(out)>3:\n",
        "        return out[:3]\n",
        "    else:\n",
        "        return out\n",
        "def correct_mispelled_t(text):\n",
        "  \n",
        "  text1=[x for x in text.split() if not(x in spacy_stopwords)]\n",
        "  text2=[x for x in text.split() if not(x in spacy_stopwords)]\n",
        "  for w1, w2 in zip(text1[:-1],text2[1:]):\n",
        "    word = Word(w1+\" \"+w2)\n",
        "    out=word.spellcheck()\n",
        "    if out[0][1]==1.0:\n",
        "      text = re.sub(word, out[0][0], text)\n",
        "  return text\n",
        "def remove_jpg(text):\n",
        "  \n",
        "  text = re.sub(r'jpg','',text)\n",
        "  return text\n",
        "def remove_punkt(text):\n",
        "  \n",
        "  text = re.sub(r'[^a-zA-Z\\s\\-\\']',' ',text)\n",
        "  return text\n",
        "def spacy_action_description(texto, matcher, nlp):\n",
        "  #texto=row.correct_Action_clean\n",
        "  doc = nlp(texto)\n",
        "  # call the matcher to find matches \n",
        "  matches = matcher(doc)\n",
        "  spans = [doc[start:end] for _, start, end in matches]\n",
        "  out =filter_spans(spans)\n",
        "  out2=[s.text for s in out ]\n",
        "  return out2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HtkJYwqrVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# correct mispelled words on text\n",
        "# Iterate on the index as is 20 times faster than apply the fucntion\n",
        "tim1= datetime.datetime.now()\n",
        "for i in datap.index:\n",
        " \n",
        "    empty_list=[]\n",
        "    texto= datap.at[i,'Action_Comment']\n",
        "    # fixing mispelled words\n",
        "    try:\n",
        "      \n",
        "      texto_clean=correct_mispelled_t(texto)\n",
        "    except:\n",
        "      texto_clean=texto\n",
        "    \n",
        "    # remove numbers and and punctuation signs and translate with spaces\n",
        "    try:\n",
        "      texto_clean=remove_jpg(texto_clean)\n",
        "      only_words= remove_punkt(texto_clean)\n",
        "    except:\n",
        "      only_words = texto_clean    \n",
        "    \n",
        "    datap.at[i,'correct_Action']=only_words \n",
        "    \n",
        "    # find keywords\n",
        "    try:\n",
        "      datap.at[i,'keywords']=apply_get_keywords(only_words, tr4w)\n",
        "    except:\n",
        "      datap.at[i,'keywords']=empty_list\n",
        "    \n",
        "    # find verbs / actions using spacy\n",
        "\n",
        "    # texto_clean=datap.at[i,'correct_Action'] \n",
        "    # try:\n",
        "    #   datap.at[i,'spacy_verb']=spacy_action_description(texto_clean,matcher,nlp)\n",
        "    # except:\n",
        "    #   datap.at[i,'spacy_verb']=empty_list\n",
        "    \n",
        "    # # find verbs using nltk\n",
        "    # try:\n",
        "    #   datap.at[i,'actions']=apply_get_actions(texto_clean,lverbs)\n",
        "    # except:\n",
        "    #   datap.at[i,'actions']=empty_list\n",
        "    # try:\n",
        "    #   datap.at[i,'verb_Adj']=apply_get_actions(texto_clean,lverbs2)\n",
        "    # except:\n",
        "    #   datap.at[i,'verb_Adj']=empty_list\n",
        "    \n",
        "    \n",
        "    if i%5000==0:\n",
        "      print (i)\n",
        "\n",
        "\n",
        "tim2= datetime.datetime.now()        \n",
        "print(tim2-tim1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgLyMe0_qr6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap.to_csv(\"/content/drive/My Drive/datasets/defects_2.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gT7v5w6qrre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap= pd.read_csv(\"/content/drive/My Drive/datasets/defects_2.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U0bBCXWqrA4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwtB9TFY8Mz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap.Element_Name.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKigdS96KOS0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datap.Feature_Name.unique()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}